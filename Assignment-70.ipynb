{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d82c7b-65d8-45b4-9a87-8b6fcf544636",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is Gradient Boosting Regression?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Gradient Boosting Regression is a machine learning algorithm that combines the principles of gradient boosting with regression tasks. It is a powerful technique for building predictive models by iteratively adding weak learners (usually decision trees) to an ensemble. In each iteration, the model is trained to correct the errors made by the previous models. Gradient Boosting Regression optimizes a loss function by minimizing the gradients of the loss with respect to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db415523-7eba-4be0-834e-4180306872ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 1.4726355328014227\n",
      "R-squared: 0.9989452132122804\n"
     ]
    }
   ],
   "source": [
    "#2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "#Ans\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators, learning_rate):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        self.residuals = y.copy()\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, self.residuals)\n",
    "            self.models.append(tree)\n",
    "\n",
    "            # Update residuals\n",
    "            predictions = tree.predict(X)\n",
    "            self.residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the gradient boosting regressor\n",
    "n_estimators = 100\n",
    "learning_rate = 0.1\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71327d80-0093-4824-9792-f9efc1fdfac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "Mean Squared Error (Best Model): 1.4726355328014227\n",
      "R-squared (Best Model): 0.9989452132122804\n"
     ]
    }
   ],
   "source": [
    "#3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "#Ans\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator):\n",
    "    def __init__(self, n_estimators, learning_rate, max_depth):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.residuals = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        self.residuals = y.copy()\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, self.residuals)\n",
    "            self.models.append(tree)\n",
    "\n",
    "            # Update residuals\n",
    "            predictions = tree.predict(X)\n",
    "            self.residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return r2_score(y, predictions)\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'max_depth': self.max_depth\n",
    "        }\n",
    "\n",
    "# Generate a small regression dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, None]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(gb_regressor, param_grid, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train a new model with the best hyperparameters\n",
    "best_gb_regressor = GradientBoostingRegressor(**best_params)\n",
    "best_gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the best model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (Best Model):\", mse)\n",
    "print(\"R-squared (Best Model):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171a8c36-2750-48f6-8409-42a74a553387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#In Gradient Boosting, a weak learner refers to a model or hypothesis that performs slightly better than random guessing. It is typically a simple and relatively low-complexity model, such as a decision tree with limited depth. The weak learner's simplicity allows it to capture only the most obvious patterns in the data, and its limitations make it prone to high bias. However, when combined with other weak learners in an ensemble, their collective strength can produce a powerful predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8ed3eb1-24da-4d7d-aa32-de3121e2cb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The intuition behind the Gradient Boosting algorithm is to sequentially add models (weak learners) to an ensemble in a way that each new model focuses on correcting the mistakes made by the previous models. At each iteration, the algorithm trains a new weak learner to predict the residuals (the differences between the target values and the current ensemble's predictions). By repeatedly fitting models to the residuals, the ensemble gradually improves its ability to capture complex patterns and reduce the overall error. The final ensemble is the combination of all weak learners weighted by a learning rate, which determines the contribution of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab344ec5-5090-41dd-9487-dd57b9ea66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The Gradient Boosting algorithm builds an ensemble of weak learners in the following steps:\n",
    "\n",
    "#1 - Initialize the ensemble by setting the initial predictions to the average of the target values.\n",
    "\n",
    "#2 - For a fixed number of iterations (or until a stopping criterion is met):\n",
    "#a. Compute the residuals by subtracting the current ensemble's predictions from the target values.\n",
    "#b. Train a weak learner (e.g., decision tree) to predict the residuals.\n",
    "#c. Update the ensemble by adding the weak learner's predictions, weighted by a learning rate.\n",
    "\n",
    "#3Repeat steps 2a to 2c, iteratively refining the ensemble's predictions by focusing on the remaining errors.\n",
    "\n",
    "#4The final ensemble is the combination of all weak learners' predictions, weighted by their corresponding learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8373960-b0f3-43bb-a29b-8c7acf4bfaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm are as follows:\n",
    "\n",
    "#1 - Given a dataset with input features X and target values y, initialize the ensemble's predictions as the average of y.\n",
    "\n",
    "#2 - For each iteration (t = 1 to T, where T is the total number of iterations):\n",
    "#a. Compute the negative gradient of the loss function with respect to the current ensemble's predictions. This gradient represents the residuals or errors to be corrected in the next iteration.\n",
    "#b. Train a weak learner (e.g., decision tree) to predict the negative gradient, using X as input and the negative gradient as the target.\n",
    "#c. Determine the learning rate (η) and the contribution of the weak learner to the ensemble. This is typically done through optimization techniques like line search or fixed step size.\n",
    "#d. Update the ensemble's predictions by adding the weak learner's predictions, scaled by the learning rate. The ensemble's predictions are now improved and reflect the corrected errors.\n",
    "\n",
    "#3 - Repeat steps 2a to 2d until the desired number of iterations is reached.\n",
    "\n",
    "#4 - The final ensemble is the sum of the initial predictions and the weighted contributions of all weak learners, forming a more accurate prediction model than any of the weak learners individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fa5d5-61d2-4f64-95e3-9ddcd2e38612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
